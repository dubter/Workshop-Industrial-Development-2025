### **Домашнее задание: Развёртывание распределённой системы логирования и хранения с резервным копированием и интеграцией Istio**

1. **Создать пользовательское веб-приложение (API)** Приложение должно реализовать следующие REST-эндпоинты:
   - `GET /` — возвращает строку `"Welcome to the custom app"`
   - `GET /status` — возвращает JSON `{"status": "ok"}`
   - `POST /log` — принимает JSON `{"message": "some log"}` и записывает его в файл `/app/logs/app.log`
   - `GET /logs` — возвращает содержимое файла `/app/logs/app.log`

   Приложение должно:
   - Писать логи в `/app/logs/app.log`
   - Использовать конфигурационные параметры (например, уровень логирования, порт, заголовок приветствия) из ConfigMap

2. **Развернуть приложение как Pod для начального теста (исключено в финальном deploy.sh)**
   - Написать Dockerfile для приложения
   - Создать Pod, монтирующий:
      - `emptyDir` volume в `/app/logs`
      - ConfigMap с настройками в `/app/config` (или через переменные окружения)
   *(Примечание: Этот шаг был частью первоначальной задачи, но исключен из финального deploy.sh в пользу развертывания через Deployment для интеграции с Istio.)*

3. **Развернуть приложение как Deployment**
   - Создать Deployment с 3 репликами
   - Настроить монтирование `emptyDir` для логов
   - Обновить Deployment, чтобы изменения в ConfigMap автоматически применялись
   - Проверить через Istio Ingress Gateway, что API работает

4. **Создать Service для балансировки нагрузки**
   - ClusterIP-сервис, направляющий трафик на поды приложения
   - Проверить через Istio Ingress Gateway: `curl http://<ingress-address>/logs` и `curl -X POST http://<ingress-address>/log -d '{"message": "test"}'`
   - Убедиться, что запросы распределяются между подами (управляется Istio DestinationRule)

5. **Развернуть DaemonSet с log-agent**
   - DaemonSet должен:
      - Быть запущен на каждом узле
      - Собирать логи приложения из подов (через `hostPath` или `emptyDir`, при наличии доступа)
      - Перенаправлять логи во stdout или сохранять локально на узле
   - Проверить, что `kubectl logs <log-agent-pod>` содержит записи из `app.log`

6. **Развернуть CronJob для архивирования логов**
   - CronJob должен запускаться раз в 10 минут
   - Команда: `tar -czf /tmp/app-logs-<timestamp>.tar.gz /app/logs/`
   - Логи берутся с сервисов приложения через HTTP API `/logs` (например, `curl`) или из общей директории, если доступна
   - Результат сохраняется в контейнере в `/tmp` (внутри пода CronJob)

7. **Создать единый bash-скрипт `deploy.sh` для автоматического развёртывания всей системы, включая Istio и Prometheus**
   - Скрипт должен:
      - **Установить Istio Control Plane** (используя `istioctl`)
      - **Включить инъекцию sidecar** для namespace приложения.
      - **Установить Prometheus Stack** (используя Helm-чарт).
      - **Собрать и опубликовать Docker образ приложения** (с метриками).
      - Создавать все необходимые **ConfigMap**, **Deployment**, **Service**, **DaemonSet**, **CronJob**.
      - Применять **конфигурации Istio** (`Gateway`, `VirtualService`, `DestinationRule`).
      - Применять **конфигурации Prometheus** (`ServiceMonitor` для приложения).
      - Использовать команды `kubectl apply -f` с заранее подготовленными YAML-файлами.
      - Ожидать готовности ключевых компонентов (Istio, Prometheus Stack, Deployment).
   - В **README.md** проекта добавьте команду для запуска скрипта из терминала.

### **Дополнительная задача 1: Интеграция с Istio**

- Настроить Istio Gateway и обеспечить внешний доступ к приложению через порт 80.
- Настроить VirtualService для маршрутизации трафика:
    - Весь входящий трафик через Gateway направляется на `app-service`.
    - Неизвестные маршруты (`/wrong`) должны возвращать 404 (обеспечивается приложением при маршрутизации на него).
    - Для маршрута `POST /log` настроить:
        - Искусственную задержку ответа (2 секунды).
        - Общий таймаут запроса (1 секунда).
        - Разрешить до 2 повторных попыток в случае сбоя (таймаута).
- Настроить DestinationRule для `app-service`:
    - Балансировка нагрузки: `LEAST_CONN`.
    - Ограничение соединений: максимум 3 TCP, максимум 5 ожидающих HTTP/1.1 запросов.
    - Включить защищенную межсервисную коммуникацию (`ISTIO_MUTUAL` TLS-режим) для трафика к `app-service` внутри mesh.

### **Дополнительная задача 2: Мониторинг через Prometheus**

#### **1. Настроить Prometheus для сбора метрик**

* Разверните Prometheus либо вручную, либо через официальный Helm-чарт `prometheus-community/kube-prometheus-stack`.
* Убедитесь, что Prometheus может:
  * собирать метрики с подов, содержащих **Envoy proxy** (через Istio)
  * автоматически обнаруживать сервисы внутри кластера через `kubernetes_sd_configs`
* Проверьте, что метрики Istio (например, `istio_requests_total`, `istio_request_duration_seconds`) доступны в интерфейсе Prometheus.

#### **2. Добавить метрики в пользовательское приложение**

* Модифицируйте приложение так, чтобы оно экспортировало метрики в формате Prometheus (например, через `/metrics`). Метрики должны включать:
  * общее количество вызовов `/log`
  * успешные и неуспешные попытки логирования
  * среднее время обработки запроса
* Используйте библиотеку для метрик на выбранном языке .

#### **3. Обеспечить сбор метрик Prometheus из приложения**

* Настройте `ServiceMonitor` или `PodMonitor`, если используется kube-prometheus-stack.
  Альтернативно — добавьте `scrape_configs` вручную, если вы используете свой Prometheus.
* Проверьте, что Prometheus может собирать метрики из `/metrics` endpoint-а приложения.
* Настройте label-селекторы для обнаружения только нужных подов.

### **Ожидаемый результат**

* Скрипт `deploy.sh` автоматически разворачивает все необходимое


---

# Решение

Данный проект представляет собой распределенную систему логирования с резервным копированием, развернутую в Kubernetes с использованием **Istio Service Mesh** для управления трафиком и **Prometheus Stack** для мониторинга.

## Компоненты системы

- **Пользовательское веб-приложение**: Микросервис, обрабатывающий REST-запросы, пишущий логи и **экспортирующий метрики Prometheus**.
- **Deployment с 3 репликами**: Обеспечивает высокую доступность приложения.
- **Service**: Kubernetes Service, предоставляющий стабильную конечную точку для подов приложения.
- **DaemonSet**: Агенты для сбора логов на каждом узле.
- **CronJob**: Периодически архивирует логи.
- **Istio Service Mesh**: Управляет входящим трафиком, обеспечивает балансировку, политики соединений, взаимный TLS и внедрение сбоев.
    - **Gateway, VirtualService, DestinationRule**: Конфигурационные ресурсы Istio.
    - **Istio Sidecar Proxies**: Автоматически внедряются в поды приложения.
- **Prometheus Stack**: Система мониторинга.
    - **Prometheus**: Собирает и хранит метрики с Istio sidecars и подов приложения (через `ServiceMonitor`).
    - **Alertmanager**: Обрабатывает алерты (не используется напрямую в этой задаче, но включен в стек).
    - **Grafana**: Визуализирует метрики с помощью дашбордов.
    - **Prometheus Operator**: Управляет экземплярами Prometheus и связанными ресурсами, такими как `ServiceMonitor`.
    - **ServiceMonitor**: Ресурс, который Operator использует для автоматической настройки Prometheus на сбор метрик с подов приложения.

## Запуск системы

Для развертывания всей системы, включая установку Istio Control Plane, установку Prometheus Stack, сборку и пуш Docker образа приложения, настройку инъекции sidecar, развертывание Kubernetes объектов приложения и применение всех конфигураций Istio и Prometheus, выполните:

```bash
./deploy.sh